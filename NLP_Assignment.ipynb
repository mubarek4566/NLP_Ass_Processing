{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da875ed",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Assignment\n",
    "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
    "- Text preprocessing\n",
    "- Tokenization and N-grams\n",
    "- Named Entity Recognition (NER)\n",
    "- Converting text into numbers (vectorization)\n",
    "- Word embeddings (for experienced learners)\n",
    "\n",
    "You can run and modify the code cells below to complete the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c0bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Specter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Specter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Specter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Specter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Specter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd99f99-d7e3-493b-8406-5bce394c8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4458f",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing\n",
    "Clean the following text by converting it to lowercase, removing punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f67aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to C:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab', download_dir='C:/nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:/nltk_data')\n",
    "nltk.data.path.append('C:/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55da019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tokens: ['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Run preprocessing\n",
    "cleaned_tokens = preprocess(text)\n",
    "print(\"Cleaned Tokens:\", cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d4be8",
   "metadata": {},
   "source": [
    "## 2. Tokenization and N-grams\n",
    "Generate bigrams (2-grams) from the cleaned tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c73f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'combines'), ('combines', 'linguistics'), ('linguistics', 'computer'), ('computer', 'science')]\n"
     ]
    }
   ],
   "source": [
    "# Generate bigrams from cleaned tokens\n",
    "bigrams = list(ngrams(cleaned_tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef82c0",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER)\n",
    "Use spaCy to perform NER on a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92d2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "Hawaii GPE\n",
      "2008 DATE\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
    "doc = nlp(sentence)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd7703",
   "metadata": {},
   "source": [
    "## 4. Converting Text to Numbers\n",
    "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f34af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Natural language processing is a part of AI.\",\n",
    "    \"AI is the future.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f41087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply CountVectorizer\n",
    "def apply_count_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554177ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply TfidfVectorizer\n",
    "def apply_tfidf_vectorizer(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aafa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both functions\n",
    "count_matrix, count_features = apply_count_vectorizer(sentences)\n",
    "tfidf_matrix, tfidf_features = apply_tfidf_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cdddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Output:\n",
      " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
      "Count Vectorizer Features: ['ai' 'future' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'of'\n",
      " 'part' 'processing' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Count Vectorizer Output:\\n\", count_matrix)\n",
    "print(\"Count Vectorizer Features:\", count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07fc7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Vectorizer Output:\n",
      " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.57735027 0.         0.         0.         0.         0.        ]\n",
      " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
      "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
      " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5628291 ]]\n",
      "TF-IDF Vectorizer Features: ['ai' 'future' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'of'\n",
      " 'part' 'processing' 'the']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTF-IDF Vectorizer Output:\\n\", tfidf_matrix)\n",
    "print(\"TF-IDF Vectorizer Features:\", tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76501072",
   "metadata": {},
   "source": [
    "## 5. Word Embeddings (Advanced)\n",
    "Use spaCy to get word vectors (embeddings) for given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a7b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
      "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
      "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
      " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
      "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
      "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
      "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
      " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
      " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
      " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
      "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
      "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
      "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
      "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
      " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
      "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
      "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
      " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
      "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
      "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
      " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
      "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
      "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
      " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
      " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
      " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
      " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
      " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
      "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
      " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
      "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
      "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
      "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
      " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
      " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
      "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
      " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
      " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
      " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
      "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
      "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
      " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
      " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
      "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
      " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
      "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
      " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
      "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
      " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
      "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n"
     ]
    }
   ],
   "source": [
    "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
    "# Uncomment below to install and load the medium model if needed.\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Example word vector\n",
    "word = nlp(\"machine\")[0]\n",
    "print(\"Vector for 'machine':\\n\", word.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cc8eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'I love machine learning.':\n",
      "[-0.83712   -0.40632   -0.24202   -0.37719    0.0055611]...\n",
      "Vector for 'Natural language processing is a part of AI.':\n",
      "[-0.66059   0.2348   -0.021227 -0.32737  -0.062493]...\n",
      "Vector for 'AI is the future.':\n",
      "[-0.83712   -0.40632   -0.24202   -0.37719    0.0055611]...\n"
     ]
    }
   ],
   "source": [
    "# Get and print vectors\n",
    "for word in sentences:\n",
    "    token = nlp(word)[0]\n",
    "    print(f\"Vector for '{word}':\\n{token.vector[:5]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
